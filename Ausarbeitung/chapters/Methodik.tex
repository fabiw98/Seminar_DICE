\chapter{Methodisches Vorgehen}
%In diesem Kapitel werden die zentralen Konzepte von DiCE betrachtet, welche zur Generierung der Counterfactuals benötigt werden. Darunter fallen Proximität, Diversität und Sparsität. Abschließend wird die Verlustfunktion als Möglichkeit der Optimierung von Counterfactuals untersucht.



Nachdem die theoretischen Grundlagen definiert wurden, befasst sich dieses Kapitel mit dem methodischen Vorgehen von DiCE. 

Die Kernidee lässt sich am besten als ein Suchprozess beschreiben: Ausgehend von einer ursprünglichen Eingabe, die zu einem unerwünschten Ergebnis geführt hat, sucht das Verfahren nach einer optimalen Menge von Alternativszenarien. Anstatt dem Anwender lediglich zu sagen, dass er abgelehnt wurde, generiert DiCE eine Erklärung, was hätte anders sein müssen, um das gewünschte Resultat zu erhalten.

%Beispiel
Ein anschauliches Beispiel hierfür ist die Ablehnung eines Kreditantrags durch ein ML-Modell. DiCE kann hier aufzeigen, dass der Kredit bewilligt worden wäre, wenn beispielsweise das Einkommen um 100\euro{} höher oder die Kreditsumme um 5000\euro{} niedriger ausgefallen wäre.

Der methodische Ablauf von DiCE lässt sich in drei Phasen unterteilen:
\begin{enumerate}
	\item \textbf{Initialisierung:} Zufällige Initialisierung der Counterfactuals.
	\item \textbf{Optimierung:} Optimierung der Counterfactuals durch iterative Minimierung der Verlustfunktion.
	\item \textbf{Nachbearbeitung:} Anpassung der Ergebnisse zur Steigerung der Sparsität.
\end{enumerate}

In den folgenden Abschnitten werden zunächst die zentralen Konzepte erläutert, bevor die mathematische Formulierung der Verlustfunktion zur Optimierung der Counterfactuals untersucht wird.
\begin{figure}[htbp]
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tikzpicture}[
		node distance=1.5cm,
		auto,
		databox/.style={
			rectangle, 
			draw=blue!50!black!80, 
			fill=blue!5!white,
			thick, 
			rounded corners, 
			text width=4.2cm, % Etwas breiter für die längeren Begriffe
			align=center, 
			minimum height=6em, % Höher für vier Zeilen Text
			font=\small
		},
		blackbox/.style={
			rectangle, 
			draw=black!80, 
			fill=gray!20,
			thick, 
			text width=4cm, 
			align=center, 
			minimum height=6em,
			font=\bfseries\small,
			drop shadow
		},
		myarrow/.style={
			->, 
			>={Latex[width=3mm,length=3mm]}, 
			thick,
			draw=gray!80
		}
		]
		
		% --- Knoten ---
		
		% 1. Eingabe-Block mit deinen präzisen Begriffen
		\node[databox] (input) {
			\textbf{Eingangswerte} \\
			\vspace{0.1cm}
			\footnotesize
			Feature-Vektor \\
			ML-Modell \\
			Ursprüngliche Klasse \\
			Gewünschte Klasse
		};
		
		% 2. Der Optimierungsprozess
		\node[blackbox, right=of input] (dice) {
			Optimierungsverfahren \\ 
			\textnormal{\footnotesize (Minimierung der Verlustfunktion)}
		};
		
		% 3. Ergebnis-Block
		\node[databox, right=of dice] (output) {
			\textbf{Ergebnis} \\
			\vspace{0.1cm}
			Menge von $k$ \\ Counterfactuals
		};
		
		% 4. Die Konzepte (Überschrift angepasst an deinen Text)
		\node [above=1cm of dice, text width=6cm, align=center, font=\small] (concepts) 
		{\textbf{Zentrale Konzepte:}\\ \textit{Proximität, Diversität, Sparsität}};
		
		% --- Pfeile ---
		\draw[myarrow] (input) -- (dice);
		\draw[myarrow] (dice) -- (output);
		\draw[myarrow] (concepts) -- (dice);
		
	\end{tikzpicture}
	}
	\caption{Methodischer Überblick der Counterfactual-Generierung: Basierend auf der ursprünglichen Eingabe und dem Modell sucht DiCE nach neuen Beispielen, welche die zentralen Konzepte bestmöglich erfüllen.}
	\label{fig:dice-methodik-overview}
\end{figure}

\section{Zentrale Konzepte}\label{chap-concepts}
%- Diversity via dpp
\textbf{Diversität} beschreibt, wie sich die generierten Counterfactuals voneinander unterscheiden.
Eine hohe Vielfältigkeit zeigt dem Anwender nicht nur mehrere Möglich-keiten zum Erreichen einer anderen Klassifikation auf, wodurch sich die Machbarkeit erhöht, sondern lässt auch größere Rückschlüsse auf das Entscheidungsverhalten des ML-Modells zu. Machbarkeit bedeutet im Kontext von DiCE, ob eine Möglichkeit für einen spezifischen Anwender theoretisch realisierbar ist. Zum Beispiel kann ein Mensch seine ethnische Herkunft nicht ändern, jedoch ist das Erlernen einer neuen Sprache möglich.
Um Diversität zu berücksichtigen, wird in DiCE das Konzept der \textbf{D}eterminantal \textbf{P}oint \textbf{P}rocesses (DPP)  verwendet, um das \textit{Subset Selection Problem} zu lösen. Das Problem beschreibt dabei die Auswahl von wenigen CFs aus einer unendlich großen Menge an möglichen Beispielen, welche zeitgleich gültig als auch divers sind. In Gleichung \ref{dpp_diversity} beschreibt $dist(c_i,c_j)$ die Distanz zwischen zwei Counterfactuals. Somit führt eine kleine Ähnlichkeit $K_{i,j}$ der CFs zu einer großen Determinante $det(K)$ und Maximierung der Diversität.\cite{mothilal2020dice, kulesza2012determinantal}
\begin{equation}\label{dpp_diversity}
	dpp\_diversity = det(K), \text{  mit }K_{i,j} = \frac{1}{1+dist(c_i,c_j)}
\end{equation}

%- Proximity
Diversität alleine ist nicht ausreichend, um einem Anwender eine Erklärung zu geben. Die generierten CFs sollten nicht nur unterschiedlich sein, sondern müssen möglichst nah an der ursprünglichen Eingabe sein.
Diese \textbf{Proximität} ist für die Machbarkeit von zentraler Bedeutung, da Anwender den meisten Nutzen aus ähnlichen Counterfactuals erhalten. 
Die Proximität eines CFs ergibt sich aus der negative Distanz zwischen dem Counterfactual $c_i$ und dem Eingabevektor $x$. 
Eine geringe Distanz resultiert in einer hohen Proximität.
Die Berechnung der mittleren Proximität einer Menge von CFs ist in Gleichung \ref{proximity} dargestellt.\cite{mothilal2020dice}
\begin{equation}\label{proximity}
	Proximity = - \frac{1}{k} \sum_{i=1}^{k}{dist(c_i,x)}
\end{equation}

%- Sparsity
Eine weitere Eigenschaft für die Machbarkeit oder auch Umsetzbarkeit der kontrafaktischen Beispiele ist die \textbf{Sparsität}. Nach der Proximität ist auch ein Counterfactual nahe an einer Eingabe, wenn jeder Vektoreintrag minimal geändert wird. Dies ist zwar mathematisch korrekt, vernachlässigt aber den Umstand der Machbarkeit für einen Anwender. Ein Counterfactual ist einfacher umzusetzen, wenn sich möglichst wenige Eigenschaften ändern. Sparsität wird über die Anzahl an unterschiedlichen Eigenschaften zwischen Eingabe und Counterfactual gemessen.\cite{mothilal2020dice}

% User Constraints
Weiterhin ist zu beachten, dass zwar ein Counterfactual ähnlich zu dem ursprünglichen Feature-Vektor sein kann, jedoch für den Anwender nicht umsetzbar ist. Aus diesem Grund muss es die Möglichkeit geben, den Wertebereich von Eigenschaften einzuschränken und zum anderen die Änderung von Eigenschaften vollständig zu verhindern.\cite{mothilal2020dice}


\section{Optimierung der Counterfactuals}

DiCE hat das Ziel eine Menge an $k$ Counterfactuals für einen Eingabevektor $x$ zu finden. Um die optimale Menge $C(x)$ zu ermitteln, wird die \textbf{Verlustfunktion} in Gleichung \ref{loss-function} verwendet und nach den Argumenten gesucht, welche die Gesamtfunktion minimieren. Die Optimierung wird über ein Gradientenabstiegsverfahren realisiert, wobei die $k$ Counterfactuals zufällig initialisiert werden.\cite{mothilal2020dice}

\begin{equation}\label{loss-function}
	\begin{split}
		C(x) &= \arg \min_{c_{1}, \dots, c_{k}} \underbrace{\frac{1}{k} \sum_{i=1}^{k}{yloss(f(c_i),y)}}_{\text{Term 1: Gültigkeit}} + \underbrace{\frac{\lambda_1}{k} \sum_{i=1}^{k}{dist(c_i,x)}}_{\text{Term 2: Proximität}}\\
		&\phantom{{}= \arg \min_{c_{1}, \dots, c_{k}} \underbrace{\frac{1}{k} \sum_{i=1}^{k}{yloss(f(c_i),y)}}_{\text{Term 1: Gültigkeit}}} - \underbrace{\lambda_2 \cdot dpp\_diversity(c_1,...,c_k)}_{\text{Term 3: Diversität}}
	\end{split}
\end{equation}

% Gültigkeit
Der erste Term beschreibt den mittleren Gültigkeitsfehler, wobei $yloss(.,.)$ die Distanz zwischen der Vorhersage $f(c_i)$ des ML-Modells für das Counterfactual $c_i$ und dem gewünschten Ergebnis $y$ misst. Die Minimierung dieses Terms führt dazu, dass die generierten Counterfactuals auch valide sind und in der gewünschten Klasse liegen.
% yloss
Für die Funktion $yloss$ wird eine Hinge-Loss-Funktion verwendet, da ein Counterfactual nur in der gewünschten Klasse liegen muss und nicht möglichst nah am Ziel $y$ liegen muss.
\cite{mothilal2020dice}

% Proximität
Die Minimierung des zweiten Terms maximiert die Proximität der Counterfactuals $c_i$, sodass diese möglichst ähnlich zum ursprünglichen Eingabevektor $x$ sind. Über den Parameter $\lambda_1$ kann der Einfluss der Proximität auf das Gesamtergebnis variiert werden. Ein großes $\lambda_1$ erhöht die Wichtigkeit der Proximität erhöht, sodass die generierten Counterfactuals ähnlicher zu der Eingabe sind.
% dist / MAD
Kategorische Features verwenden eine 0/1-Funktion, sodass in einem Counterfactual geänderte Features mit 1 und unveränderte Features mit 0 gekennzeichnet werden. Kontinuierliche Features verwenden eine $l_1$-Distanz, welche durch den Median der absoluten Abweichungen (MAD) dividiert wird. Dies skaliert die Distanz, um Änderungen fair zu gewichten.
\cite{mothilal2020dice}

% Diversität
Um die Gesamtfunktion zu minimieren, muss der dritte Term maximiert werden. Die Gewichtung der Diversität wird mithilfe von $\lambda_2$ festgelegt, wobei eine Erhöhung des Parameters die Unterschiede zwischen den Counterfactuals erhöht. \cite{mothilal2020dice}
\\
\\
% Sparsität
Die Verlustfunktion bestimmt eine optimale Menge an Counterfactuals bezüglich Gültigkeit, Proximität und Diversität, vernachlässigt jedoch die Sparsität. Aus diesem Grund erfolgt eine Nachbearbeitung der CFs, um die Sparsität zu erhöhen. Dazu wird ein Greedy-Algorithmus verwendet, welcher jedes Counterfactuals einzeln betrachtet. Der Algorithmus versucht jedes kontinuierliche Feature des CFs $c_i$ auf den Wert des ursprünglichen Eingabevektors $x$ zurückzusetzen, ohne dass sich die Vorhersage des ML-Modells $f(c_i)$ ändert. Dabei wird das Zurücksetzen zuerst bei Features mit geringen Änderungen versucht.\cite{mothilal2020dice}


